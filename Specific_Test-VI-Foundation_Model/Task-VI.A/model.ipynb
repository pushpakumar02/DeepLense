{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac15c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class_names = ['no_sub', 'cdm', 'axion']  # Global class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ccc14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LensDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_names, mode='pretrain', mask_ratio=0.75):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.mode = mode\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.target_size = (64, 64)\n",
    "        \n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label_idx, class_name in enumerate(class_names):\n",
    "            class_dir = self.root_dir / class_name\n",
    "            files = [f for f in class_dir.glob('*.npy') if self._is_valid_file(f)]\n",
    "            print(f\"Found {len(files)} valid files in {class_name}\")\n",
    "            self.file_paths.extend(files)\n",
    "            self.labels.extend([label_idx] * len(files))\n",
    "            \n",
    "        if mode == 'pretrain':\n",
    "            self.file_paths = [f for f in self.file_paths if 'no_sub' in str(f).lower()]\n",
    "            print(f\"Using {len(self.file_paths)} no_sub files for pretraining\")\n",
    "\n",
    "    def _is_valid_file(self, file_path):\n",
    "        \"\"\"Updated validation to handle Axion's special format\"\"\"\n",
    "        try:\n",
    "            data = np.load(file_path, allow_pickle=True)\n",
    "            if isinstance(data, np.ndarray):\n",
    "                if data.dtype == object and data.shape == (2,):  # Axion format\n",
    "                    return isinstance(data[0], np.ndarray) and data[0].shape == (64, 64)\n",
    "                return data.dtype.kind in {'f', 'i', 'u', 'b'} and data.size > 0\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            loaded = np.load(self.file_paths[idx], allow_pickle=True)\n",
    "            \n",
    "            # Handle Axion's special format\n",
    "            if loaded.dtype == object and loaded.shape == (2,):\n",
    "                img = loaded[0]  # Extract the image array\n",
    "            else:\n",
    "                img = loaded\n",
    "                \n",
    "            img = torch.tensor(img, dtype=torch.float32)\n",
    "            \n",
    "            # Convert to 3 channels if needed\n",
    "            if img.ndim == 2:\n",
    "                img = img.unsqueeze(0).repeat(3, 1, 1)\n",
    "            elif img.shape[0] == 1:\n",
    "                img = img.repeat(3, 1, 1)\n",
    "            \n",
    "            # Resize if needed\n",
    "            if img.shape[1:] != self.target_size:\n",
    "                img = F.interpolate(img.unsqueeze(0), size=self.target_size, mode='bilinear').squeeze(0)\n",
    "            \n",
    "            if self.mode == 'pretrain':\n",
    "                _, H, W = img.shape\n",
    "                num_pixels = H * W\n",
    "                mask = torch.zeros(num_pixels)\n",
    "                mask[:int(num_pixels * self.mask_ratio)] = 1\n",
    "                mask = mask[torch.randperm(num_pixels)].reshape(1, H, W)\n",
    "                masked_img = img * (1 - mask)\n",
    "                return masked_img, img, mask\n",
    "            else:\n",
    "                return img, torch.tensor(self.labels[idx])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.file_paths[idx]}: {e}\")\n",
    "            return self[np.random.randint(0, len(self))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180ec21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104c89e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc1ba2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_mae(model, train_loader, epochs=5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for masked_imgs, target_imgs, _ in tqdm(train_loader, desc=f'Pretrain Epoch {epoch+1}/{epochs}'):\n",
    "            masked_imgs = masked_imgs.to(device)\n",
    "            target_imgs = target_imgs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(masked_imgs)\n",
    "            loss = criterion(outputs, target_imgs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712aaeb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, test_loader, epochs=10):\n",
    "    model.train()\n",
    "    # Class weights: Higher weight for cdm (class 1)\n",
    "    class_weights = torch.tensor([1.0, 2.0, 1.0]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f'Finetune Epoch {epoch+1}/{epochs}'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_accuracy = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        # Validation\n",
    "        val_accuracy = evaluate_model(model, test_loader, verbose=False)\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_classifier.pth')\n",
    "            print(f\"New best model saved with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf2044",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append(outputs.argmax(dim=1).cpu())\n",
    "    \n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    \n",
    "    # ROC AUC and Curve\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = all_probs.shape[1]\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n",
    "        roc_auc[i] = roc_auc_score((all_labels == i).astype(int), all_probs[:, i])\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['blue', 'red', 'green'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of {class_names[i]} (AUC = {roc_auc[i]:0.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Confusion Matrix with values\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    tick_marks = np.arange(n_classes)\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    avg_auc = np.mean(list(roc_auc.values()))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"{class_names[i]} AUC: {roc_auc[i]:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb03d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Dataset setup\n",
    "    data_dir = Path(\"Dataset\")\n",
    "    \n",
    "    # Pretrain MAE\n",
    "    print(\"\\nPretraining MAE...\")\n",
    "    pretrain_dataset = LensDataset(data_dir, class_names, 'pretrain')\n",
    "    pretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    mae = MAE().to(device)\n",
    "    train_mae(mae, pretrain_loader, epochs=5)\n",
    "    \n",
    "    # Save the pretrained weights for Task VI.B\n",
    "    torch.save(mae.state_dict(), 'mae_pretrained.pth')\n",
    "    print(\"\\nSaved pretrained MAE weights to mae_pretrained.pth\")\n",
    "    \n",
    "    # Finetune Classifier\n",
    "    print(\"\\nFinetuning Classifier...\")\n",
    "    full_dataset = LensDataset(data_dir, class_names, 'finetune')\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    train_dataset, test_dataset = random_split(full_dataset, [train_size, len(full_dataset)-train_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, num_workers=0)\n",
    "    \n",
    "    classifier = Classifier(mae.encoder).to(device)\n",
    "    train_classifier(classifier, train_loader, test_loader, epochs=10)\n",
    "    \n",
    "    # Final Evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    evaluate_model(classifier, test_loader)\n",
    "    \n",
    "    print(\"\\nTraining complete! Saved the following files:\")\n",
    "    print(\"- mae_pretrained.pth (pretrained MAE weights for Task VI.B)\")\n",
    "    print(\"- best_classifier.pth (best classifier weights)\")\n",
    "    print(\"- roc_curve.png (ROC curve visualization)\")\n",
    "    print(\"- confusion_matrix.png (confusion matrix with values)\")\n",
    "    print(\"- training_curves.png (training loss and accuracy curves)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140f1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
