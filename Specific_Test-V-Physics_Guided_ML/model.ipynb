{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a60cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('mps')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3c2f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ========== PHYSICS LAYER ==========\n",
    "class LensingPhysicsLayer(nn.Module):\n",
    "    \"\"\"Implements simplified gravitational lensing equation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.theta_E = nn.Parameter(torch.tensor(1.0))  # Trainable Einstein radius\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        y_coords = torch.linspace(-1, 1, H, device=x.device)\n",
    "        x_coords = torch.linspace(-1, 1, W, device=x.device)\n",
    "        yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "        r = torch.sqrt(xx**2 + yy**2) + 1e-6\n",
    "        deflection = self.theta_E**2 / r\n",
    "        return x * deflection.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77538a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class LensDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.files = list(self.root_dir.rglob('*.npy'))\n",
    "        self.labels = [self.get_label(f) for f in self.files]\n",
    "        self.transform = transform or self.default_transform()\n",
    "    \n",
    "    def get_label(self, file):\n",
    "        file_str = str(file).lower()\n",
    "        if 'vort' in file_str: return 0\n",
    "        elif 'sphere' in file_str: return 1\n",
    "        else: return 2\n",
    "    \n",
    "    def default_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.unsqueeze(0) if x.dim() == 2 else x),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = np.load(self.files[idx], allow_pickle=True)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        if img.dim() == 2: img = img.unsqueeze(0)\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img.to(device), torch.tensor(self.labels[idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da197a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "project_root = Path('/Users/pushpakumar/Projects/GSoC25_DeepLense-Gravitational Lens Finding /Common_Test-I-MultiClass')\n",
    "train_data = LensDataset(project_root / 'dataset'/'train')\n",
    "val_data = LensDataset(project_root / 'dataset'/'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3bd4a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05670b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load base model\n",
    "def load_resnet_from_local(weights_path):\n",
    "    model = models.resnet18(weights=None)\n",
    "    state_dict = torch.load(weights_path)\n",
    "    if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_resnet_from_local(\"/Users/pushpakumar/Downloads/resnet18-f37072fd.pth\")\n",
    "with torch.no_grad():\n",
    "    base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    base_model.conv1.weight = nn.Parameter(base_model.conv1.weight.mean(dim=1, keepdim=True))\n",
    "base_model.fc = nn.Linear(base_model.fc.in_features, 3)\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe19ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PHYSICS MODEL ==========\n",
    "physics_model = nn.Sequential(\n",
    "    LensingPhysicsLayer(),\n",
    "    base_model\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2254b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "class_weights = torch.tensor([1.0, 1.5, 1.2]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(physics_model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fa815",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct = 0.0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = 100 * correct / len(val_loader.dataset)\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}:')\n",
    "        print(f'Train Loss: {epoch_loss/len(train_loader):.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(classification_report(all_labels, all_preds, target_names=['Vort', 'Sphere', 'No Substructure']))\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_physics_model.pth')\n",
    "            print(f'New best model saved with val acc: {best_val_acc:.2f}%')\n",
    "        \n",
    "        if epoch > 7 and val_acc < 90:\n",
    "            print(\"Early stopping - model not converging\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ec998",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Vort', 'Sphere', 'No Substructure'],\n",
    "                yticklabels=['Vort', 'Sphere', 'No Substructure'])\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in range(3):\n",
    "        fpr, tpr, _ = roc_curve(np.array(y_true) == i, np.array(y_probs)[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343ac51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_training_plots(train_losses, val_losses, val_accs):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4644b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_physics_effect(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        sample = next(iter(val_loader))[0][0:1]\n",
    "        phys_effect = model[0](sample).cpu()\n",
    "        plt.figure()\n",
    "        plt.imshow(phys_effect.squeeze(), cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.savefig('physics_effect.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, val_losses, val_accs = train_model(physics_model, train_loader, val_loader, epochs=10)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining stopped early\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all outputs\n",
    "plt.close('all')\n",
    "generate_training_plots(train_losses, val_losses, val_accs)\n",
    "visualize_physics_effect(physics_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ab649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate best model\n",
    "physics_model.load_state_dict(torch.load('best_physics_model.pth'))\n",
    "evaluate_model(physics_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll results saved:\")\n",
    "print(\"1. training_curves.png\\n2. physics_effect.png\\n3. confusion_matrix.png\\n4. roc_curve.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
