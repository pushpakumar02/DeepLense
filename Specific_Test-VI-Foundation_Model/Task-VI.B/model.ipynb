{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c7952",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31122150",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir):\n",
    "        self.lr_dir = Path(lr_dir)\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.file_names = sorted([f.name for f in self.lr_dir.glob('*.npy')])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr = np.load(self.lr_dir/self.file_names[idx]).squeeze().astype(np.float32)\n",
    "        hr = np.load(self.hr_dir/self.file_names[idx]).squeeze().astype(np.float32)\n",
    "        return (\n",
    "            torch.from_numpy(lr).unsqueeze(0).float(),\n",
    "            torch.from_numpy(hr).unsqueeze(0).float()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485f110",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder (matches MAE exactly)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Decoder with proper upsampling\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(size=(150, 150), mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 1, 3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)\n",
    "    \n",
    "    def load_pretrained_weights(self, path):\n",
    "        \"\"\"Load MAE encoder weights with proper channel conversion\"\"\"\n",
    "        try:\n",
    "            mae_weights = torch.load(path, map_location=device)\n",
    "            model_dict = self.state_dict()\n",
    "            \n",
    "            # First conv layer (3-channel -> 1-channel)\n",
    "            if 'encoder.0.weight' in mae_weights:\n",
    "                model_dict['encoder.0.weight'] = mae_weights['encoder.0.weight'].mean(dim=1, keepdim=True)\n",
    "                model_dict['encoder.0.bias'] = mae_weights['encoder.0.bias']\n",
    "                loaded = 2\n",
    "            else:\n",
    "                raise RuntimeError(\"Missing first conv layer\")\n",
    "            \n",
    "            # Transfer remaining compatible weights\n",
    "            weight_map = {\n",
    "                'encoder.1.weight': 'encoder.1.weight',\n",
    "                'encoder.1.bias': 'encoder.1.bias',\n",
    "                'encoder.4.weight': 'encoder.4.weight',\n",
    "                'encoder.4.bias': 'encoder.4.bias',\n",
    "                'encoder.5.weight': 'encoder.5.weight',\n",
    "                'encoder.5.bias': 'encoder.5.bias'\n",
    "            }\n",
    "            \n",
    "            for mae_name, sr_name in weight_map.items():\n",
    "                if mae_name in mae_weights and sr_name in model_dict:\n",
    "                    model_dict[sr_name] = mae_weights[mae_name]\n",
    "                    loaded += 1\n",
    "            \n",
    "            if loaded < 6:\n",
    "                raise RuntimeError(f\"Only {loaded}/8 weights loaded\")\n",
    "                \n",
    "            self.load_state_dict(model_dict, strict=False)\n",
    "            print(f\"âœ… Successfully loaded {loaded} pretrained layers\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Weight loading failed: {str(e)}\")\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c095db",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Create output directory (clears previous best images)\n",
    "    os.makedirs('results/best_comparisons', exist_ok=True)\n",
    "    for f in os.listdir('results/best_comparisons'):\n",
    "        os.remove(os.path.join('results/best_comparisons', f))\n",
    "    \n",
    "    # Dataset setup\n",
    "    dataset = SRDataset('Dataset/LR', 'Dataset/HR')\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=16)\n",
    "    \n",
    "    # Model initialization\n",
    "    model = SuperResolutionNet().to(device)\n",
    "    model.load_pretrained_weights('../Task-VI.A/mae_pretrained.pth')\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    best_psnr = 0\n",
    "    no_improve = 0\n",
    "    patience = 5\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for lr, hr in tqdm(train_loader, desc=f'Epoch {epoch+1}/30'):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            sr = model(lr)\n",
    "            loss = criterion(sr, hr)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"Train Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        print(f\"MSE: {val_metrics['mse']:.6f} | PSNR: {val_metrics['psnr']:.2f} dB | SSIM: {val_metrics['ssim']:.4f}\")\n",
    "        \n",
    "        if val_metrics['psnr'] > best_psnr:\n",
    "            best_psnr = val_metrics['psnr']\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_sr_model.pth')\n",
    "            print(f\"New best model (PSNR: {best_psnr:.2f} dB)\")\n",
    "            \n",
    "            # Generate new best comparison (replaces previous)\n",
    "            generate_best_comparison(model, val_loader, epoch+1)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"No improvement ({no_improve}/{patience})\")\n",
    "    \n",
    "    print(\"\\nTraining complete! Best model saved as 'best_sr_model.pth'\")\n",
    "    print(f\"Best comparison images saved in 'results/best_comparisons/' (PSNR: {best_psnr:.2f} dB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527c1c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    metrics = {'mse': 0, 'psnr': 0, 'ssim': 0}\n",
    "    with torch.no_grad():\n",
    "        for lr, hr in loader:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr = model(lr)\n",
    "            \n",
    "            # Ensure output matches target size\n",
    "            if sr.shape[-2:] != hr.shape[-2:]:\n",
    "                sr = F.interpolate(sr, size=hr.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = F.mse_loss(sr, hr)\n",
    "            metrics['mse'] += mse.item() * lr.size(0)\n",
    "            psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "            metrics['psnr'] += psnr.item() * lr.size(0)\n",
    "            \n",
    "            for i in range(sr.shape[0]):\n",
    "                metrics['ssim'] += ssim(\n",
    "                    sr[i].cpu().numpy().squeeze(),\n",
    "                    hr[i].cpu().numpy().squeeze(),\n",
    "                    data_range=1.0\n",
    "                )\n",
    "    return {k: v/len(loader.dataset) for k,v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66981c83",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_best_comparison(model, loader, epoch, num_samples=3):\n",
    "    \"\"\"Generates and saves ONLY the current best model comparisons\"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs('results/best_comparisons', exist_ok=True)\n",
    "    \n",
    "    # Clear previous best images\n",
    "    for f in os.listdir('results/best_comparisons'):\n",
    "        os.remove(os.path.join('results/best_comparisons', f))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (lr, hr) in enumerate(loader):\n",
    "            if i >= num_samples: break\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr = model(lr)\n",
    "            \n",
    "            if sr.shape[-2:] != hr.shape[-2:]:\n",
    "                sr = F.interpolate(sr, size=hr.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "            # Calculate metrics for this sample\n",
    "            mse = F.mse_loss(sr, hr).item()\n",
    "            psnr = 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "            ssim_val = ssim(\n",
    "                sr[0].cpu().numpy().squeeze(),\n",
    "                hr[0].cpu().numpy().squeeze(),\n",
    "                data_range=1.0\n",
    "            )\n",
    "            \n",
    "            # Create comparison figure\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            # Low Res\n",
    "            axes[0].imshow(lr[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "            axes[0].set_title(f'Low Resolution\\n{lr.shape[-2]}x{lr.shape[-1]}')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Super-Resolved\n",
    "            axes[1].imshow(sr[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "            axes[1].set_title(f'Super-Resolved\\nPSNR: {psnr:.2f} dB | SSIM: {ssim_val:.4f}')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # High Res\n",
    "            axes[2].imshow(hr[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "            axes[2].set_title(f'High Resolution\\n{hr.shape[-2]}x{hr.shape[-1]} (Ground Truth)')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'results/best_comparisons/best_epoch_{epoch}_sample_{i}.png', bbox_inches='tight', dpi=300)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c021f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
